{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unet_mnist_0429.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJw_JI-UIF6",
        "colab_type": "text"
      },
      "source": [
        "copied from:\\\n",
        "https://github.com/jakeret/tf_unet/blob/master/tf_unet/unet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxMq4WQPUEw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf_unet is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "# \n",
        "# tf_unet is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU General Public License for more details.\n",
        "# \n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with tf_unet.  If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "\n",
        "'''\n",
        "Created on Jul 28, 2016\n",
        "author: jakeret\n",
        "'''\n",
        "from __future__ import print_function, division, absolute_import, unicode_literals\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_unet import util\n",
        "from tf_unet.layers import (weight_variable, weight_variable_devonc, bias_variable,\n",
        "                            conv2d, deconv2d, max_pool, crop_and_concat, pixel_wise_softmax,\n",
        "                            cross_entropy)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
        "\n",
        "\n",
        "def create_conv_net(x, keep_prob, channels, n_class, layers=3, features_root=16, filter_size=3, pool_size=2,\n",
        "                    summaries=True):\n",
        "    \"\"\"\n",
        "    Creates a new convolutional unet for the given parametrization.\n",
        "    :param x: input tensor, shape [?,nx,ny,channels]\n",
        "    :param keep_prob: dropout probability tensor\n",
        "    :param channels: number of channels in the input image\n",
        "    :param n_class: number of output labels\n",
        "    :param layers: number of layers in the net\n",
        "    :param features_root: number of features in the first layer\n",
        "    :param filter_size: size of the convolution filter\n",
        "    :param pool_size: size of the max pooling operation\n",
        "    :param summaries: Flag if summaries should be created\n",
        "    \"\"\"\n",
        "\n",
        "    logging.info(\n",
        "        \"Layers {layers}, features {features}, filter size {filter_size}x{filter_size}, pool size: {pool_size}x{pool_size}\".format(\n",
        "            layers=layers,\n",
        "            features=features_root,\n",
        "            filter_size=filter_size,\n",
        "            pool_size=pool_size))\n",
        "\n",
        "    # Placeholder for the input image\n",
        "    with tf.name_scope(\"preprocessing\"):\n",
        "        nx = tf.shape(x)[1]\n",
        "        ny = tf.shape(x)[2]\n",
        "        x_image = tf.reshape(x, tf.stack([-1, nx, ny, channels]))\n",
        "        in_node = x_image\n",
        "        batch_size = tf.shape(x_image)[0]\n",
        "\n",
        "    weights = []\n",
        "    biases = []\n",
        "    convs = []\n",
        "    pools = OrderedDict()\n",
        "    deconv = OrderedDict()\n",
        "    dw_h_convs = OrderedDict()\n",
        "    up_h_convs = OrderedDict()\n",
        "\n",
        "    in_size = 1000\n",
        "    size = in_size\n",
        "    # down layers\n",
        "    for layer in range(0, layers):\n",
        "        with tf.name_scope(\"down_conv_{}\".format(str(layer))):\n",
        "            features = 2 ** layer * features_root\n",
        "            stddev = np.sqrt(2 / (filter_size ** 2 * features))\n",
        "            if layer == 0:\n",
        "                w1 = weight_variable([filter_size, filter_size, channels, features], stddev, name=\"w1\")\n",
        "            else:\n",
        "                w1 = weight_variable([filter_size, filter_size, features // 2, features], stddev, name=\"w1\")\n",
        "\n",
        "            w2 = weight_variable([filter_size, filter_size, features, features], stddev, name=\"w2\")\n",
        "            b1 = bias_variable([features], name=\"b1\")\n",
        "            b2 = bias_variable([features], name=\"b2\")\n",
        "\n",
        "            conv1 = conv2d(in_node, w1, b1, keep_prob)\n",
        "            tmp_h_conv = tf.nn.relu(conv1)\n",
        "            conv2 = conv2d(tmp_h_conv, w2, b2, keep_prob)\n",
        "            dw_h_convs[layer] = tf.nn.relu(conv2)\n",
        "\n",
        "            weights.append((w1, w2))\n",
        "            biases.append((b1, b2))\n",
        "            convs.append((conv1, conv2))\n",
        "\n",
        "            size -= 2 * 2 * (filter_size // 2) # valid conv\n",
        "            if layer < layers - 1:\n",
        "                pools[layer] = max_pool(dw_h_convs[layer], pool_size)\n",
        "                in_node = pools[layer]\n",
        "                size /= pool_size\n",
        "\n",
        "    in_node = dw_h_convs[layers - 1]\n",
        "\n",
        "    # up layers\n",
        "    for layer in range(layers - 2, -1, -1):\n",
        "        with tf.name_scope(\"up_conv_{}\".format(str(layer))):\n",
        "            features = 2 ** (layer + 1) * features_root\n",
        "            stddev = np.sqrt(2 / (filter_size ** 2 * features))\n",
        "\n",
        "            wd = weight_variable_devonc([pool_size, pool_size, features // 2, features], stddev, name=\"wd\")\n",
        "            bd = bias_variable([features // 2], name=\"bd\")\n",
        "            h_deconv = tf.nn.relu(deconv2d(in_node, wd, pool_size) + bd)\n",
        "            h_deconv_concat = crop_and_concat(dw_h_convs[layer], h_deconv)\n",
        "            deconv[layer] = h_deconv_concat\n",
        "\n",
        "            w1 = weight_variable([filter_size, filter_size, features, features // 2], stddev, name=\"w1\")\n",
        "            w2 = weight_variable([filter_size, filter_size, features // 2, features // 2], stddev, name=\"w2\")\n",
        "            b1 = bias_variable([features // 2], name=\"b1\")\n",
        "            b2 = bias_variable([features // 2], name=\"b2\")\n",
        "\n",
        "            conv1 = conv2d(h_deconv_concat, w1, b1, keep_prob)\n",
        "            h_conv = tf.nn.relu(conv1)\n",
        "            conv2 = conv2d(h_conv, w2, b2, keep_prob)\n",
        "            in_node = tf.nn.relu(conv2)\n",
        "            up_h_convs[layer] = in_node\n",
        "\n",
        "            weights.append((w1, w2))\n",
        "            biases.append((b1, b2))\n",
        "            convs.append((conv1, conv2))\n",
        "\n",
        "            size *= pool_size\n",
        "            size -= 2 * 2 * (filter_size // 2) # valid conv\n",
        "\n",
        "    # Output Map\n",
        "    with tf.name_scope(\"output_map\"):\n",
        "        weight = weight_variable([1, 1, features_root, n_class], stddev)\n",
        "        bias = bias_variable([n_class], name=\"bias\")\n",
        "        conv = conv2d(in_node, weight, bias, tf.constant(1.0))\n",
        "        output_map = tf.nn.relu(conv)\n",
        "        up_h_convs[\"out\"] = output_map\n",
        "\n",
        "    if summaries:\n",
        "        with tf.name_scope(\"summaries\"):\n",
        "            for i, (c1, c2) in enumerate(convs):\n",
        "                tf.summary.image('summary_conv_%02d_01' % i, get_image_summary(c1))\n",
        "                tf.summary.image('summary_conv_%02d_02' % i, get_image_summary(c2))\n",
        "\n",
        "            for k in pools.keys():\n",
        "                tf.summary.image('summary_pool_%02d' % k, get_image_summary(pools[k]))\n",
        "\n",
        "            for k in deconv.keys():\n",
        "                tf.summary.image('summary_deconv_concat_%02d' % k, get_image_summary(deconv[k]))\n",
        "\n",
        "            for k in dw_h_convs.keys():\n",
        "                tf.summary.histogram(\"dw_convolution_%02d\" % k + '/activations', dw_h_convs[k])\n",
        "\n",
        "            for k in up_h_convs.keys():\n",
        "                tf.summary.histogram(\"up_convolution_%s\" % k + '/activations', up_h_convs[k])\n",
        "\n",
        "    variables = []\n",
        "    for w1, w2 in weights:\n",
        "        variables.append(w1)\n",
        "        variables.append(w2)\n",
        "\n",
        "    for b1, b2 in biases:\n",
        "        variables.append(b1)\n",
        "        variables.append(b2)\n",
        "\n",
        "    return output_map, variables, int(in_size - size)\n",
        "\n",
        "\n",
        "class Unet(object):\n",
        "    \"\"\"\n",
        "    A unet implementation\n",
        "    :param channels: number of channels in the input image\n",
        "    :param n_class: number of output labels\n",
        "    :param cost: (optional) name of the cost function. Default is 'cross_entropy'\n",
        "    :param cost_kwargs: (optional) kwargs passed to the cost function. See Unet._get_cost for more options\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, n_class, cost=\"cross_entropy\", cost_kwargs={}, **kwargs):\n",
        "        tf.reset_default_graph()\n",
        "\n",
        "        self.n_class = n_class\n",
        "        self.summaries = kwargs.get(\"summaries\", True)\n",
        "\n",
        "        self.x = tf.placeholder(\"float\", shape=[None, None, None, channels], name=\"x\")\n",
        "        self.y = tf.placeholder(\"float\", shape=[None, None, None, n_class], name=\"y\")\n",
        "        self.keep_prob = tf.placeholder(tf.float32, name=\"dropout_probability\")  # dropout (keep probability)\n",
        "\n",
        "        logits, self.variables, self.offset = create_conv_net(self.x, self.keep_prob, channels, n_class, **kwargs)\n",
        "\n",
        "        self.cost = self._get_cost(logits, cost, cost_kwargs)\n",
        "\n",
        "        self.gradients_node = tf.gradients(self.cost, self.variables)\n",
        "\n",
        "        with tf.name_scope(\"cross_entropy\"):\n",
        "            self.cross_entropy = cross_entropy(tf.reshape(self.y, [-1, n_class]),\n",
        "                                               tf.reshape(pixel_wise_softmax(logits), [-1, n_class]))\n",
        "\n",
        "        with tf.name_scope(\"results\"):\n",
        "            self.predicter = pixel_wise_softmax(logits)\n",
        "            self.correct_pred = tf.equal(tf.argmax(self.predicter, 3), tf.argmax(self.y, 3))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
        "\n",
        "    def _get_cost(self, logits, cost_name, cost_kwargs):\n",
        "        \"\"\"\n",
        "        Constructs the cost function, either cross_entropy, weighted cross_entropy or dice_coefficient.\n",
        "        Optional arguments are:\n",
        "        class_weights: weights for the different classes in case of multi-class imbalance\n",
        "        regularizer: power of the L2 regularizers added to the loss function\n",
        "        \"\"\"\n",
        "\n",
        "        with tf.name_scope(\"cost\"):\n",
        "            flat_logits = tf.reshape(logits, [-1, self.n_class])\n",
        "            flat_labels = tf.reshape(self.y, [-1, self.n_class])\n",
        "            if cost_name == \"cross_entropy\":\n",
        "                class_weights = cost_kwargs.pop(\"class_weights\", None)\n",
        "\n",
        "                if class_weights is not None:\n",
        "                    class_weights = tf.constant(np.array(class_weights, dtype=np.float32))\n",
        "\n",
        "                    weight_map = tf.multiply(flat_labels, class_weights)\n",
        "                    weight_map = tf.reduce_sum(weight_map, axis=1)\n",
        "\n",
        "                    loss_map = tf.nn.softmax_cross_entropy_with_logits_v2(logits=flat_logits,\n",
        "                                                                          labels=flat_labels)\n",
        "                    weighted_loss = tf.multiply(loss_map, weight_map)\n",
        "\n",
        "                    loss = tf.reduce_mean(weighted_loss)\n",
        "\n",
        "                else:\n",
        "                    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=flat_logits,\n",
        "                                                                                     labels=flat_labels))\n",
        "            elif cost_name == \"dice_coefficient\":\n",
        "                eps = 1e-5\n",
        "                prediction = pixel_wise_softmax(logits)\n",
        "                intersection = tf.reduce_sum(prediction * self.y)\n",
        "                union = eps + tf.reduce_sum(prediction) + tf.reduce_sum(self.y)\n",
        "                loss = -(2 * intersection / (union))\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Unknown cost function: \" % cost_name)\n",
        "\n",
        "            regularizer = cost_kwargs.pop(\"regularizer\", None)\n",
        "            if regularizer is not None:\n",
        "                regularizers = sum([tf.nn.l2_loss(variable) for variable in self.variables])\n",
        "                loss += (regularizer * regularizers)\n",
        "\n",
        "            return loss\n",
        "\n",
        "    def predict(self, model_path, x_test):\n",
        "        \"\"\"\n",
        "        Uses the model to create a prediction for the given data\n",
        "        :param model_path: path to the model checkpoint to restore\n",
        "        :param x_test: Data to predict on. Shape [n, nx, ny, channels]\n",
        "        :returns prediction: The unet prediction Shape [n, px, py, labels] (px=nx-self.offset/2)\n",
        "        \"\"\"\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        with tf.Session() as sess:\n",
        "            # Initialize variables\n",
        "            sess.run(init)\n",
        "\n",
        "            # Restore model weights from previously saved model\n",
        "            self.restore(sess, model_path)\n",
        "\n",
        "            y_dummy = np.empty((x_test.shape[0], x_test.shape[1], x_test.shape[2], self.n_class))\n",
        "            prediction = sess.run(self.predicter, feed_dict={self.x: x_test, self.y: y_dummy, self.keep_prob: 1.})\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def save(self, sess, model_path):\n",
        "        \"\"\"\n",
        "        Saves the current session to a checkpoint\n",
        "        :param sess: current session\n",
        "        :param model_path: path to file system location\n",
        "        \"\"\"\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        save_path = saver.save(sess, model_path)\n",
        "        return save_path\n",
        "\n",
        "    def restore(self, sess, model_path):\n",
        "        \"\"\"\n",
        "        Restores a session from a checkpoint\n",
        "        :param sess: current session instance\n",
        "        :param model_path: path to file system checkpoint location\n",
        "        \"\"\"\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(sess, model_path)\n",
        "        logging.info(\"Model restored from file: %s\" % model_path)\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    \"\"\"\n",
        "    Trains a unet instance\n",
        "    :param net: the unet instance to train\n",
        "    :param batch_size: size of training batch\n",
        "    :param verification_batch_size: size of verification batch\n",
        "    :param norm_grads: (optional) true if normalized gradients should be added to the summaries\n",
        "    :param optimizer: (optional) name of the optimizer to use (momentum or adam)\n",
        "    :param opt_kwargs: (optional) kwargs passed to the learning rate (momentum opt) and to the optimizer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, net, batch_size=1, verification_batch_size = 4, norm_grads=False, optimizer=\"momentum\", opt_kwargs={}):\n",
        "        self.net = net\n",
        "        self.batch_size = batch_size\n",
        "        self.verification_batch_size = verification_batch_size\n",
        "        self.norm_grads = norm_grads\n",
        "        self.optimizer = optimizer\n",
        "        self.opt_kwargs = opt_kwargs\n",
        "\n",
        "    def _get_optimizer(self, training_iters, global_step):\n",
        "        if self.optimizer == \"momentum\":\n",
        "            learning_rate = self.opt_kwargs.pop(\"learning_rate\", 0.2)\n",
        "            decay_rate = self.opt_kwargs.pop(\"decay_rate\", 0.95)\n",
        "            momentum = self.opt_kwargs.pop(\"momentum\", 0.2)\n",
        "\n",
        "            self.learning_rate_node = tf.train.exponential_decay(learning_rate=learning_rate,\n",
        "                                                                 global_step=global_step,\n",
        "                                                                 decay_steps=training_iters,\n",
        "                                                                 decay_rate=decay_rate,\n",
        "                                                                 staircase=True)\n",
        "\n",
        "            optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate_node, momentum=momentum,\n",
        "                                                   **self.opt_kwargs).minimize(self.net.cost,\n",
        "                                                                               global_step=global_step)\n",
        "        elif self.optimizer == \"adam\":\n",
        "            learning_rate = self.opt_kwargs.pop(\"learning_rate\", 0.001)\n",
        "            self.learning_rate_node = tf.Variable(learning_rate, name=\"learning_rate\")\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_node,\n",
        "                                               **self.opt_kwargs).minimize(self.net.cost,\n",
        "                                                                           global_step=global_step)\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def _initialize(self, training_iters, output_path, restore, prediction_path):\n",
        "        global_step = tf.Variable(0, name=\"global_step\")\n",
        "\n",
        "        self.norm_gradients_node = tf.Variable(tf.constant(0.0, shape=[len(self.net.gradients_node)]), name=\"norm_gradients\")\n",
        "\n",
        "        if self.net.summaries and self.norm_grads:\n",
        "            tf.summary.histogram('norm_grads', self.norm_gradients_node)\n",
        "\n",
        "        tf.summary.scalar('loss', self.net.cost)\n",
        "        tf.summary.scalar('cross_entropy', self.net.cross_entropy)\n",
        "        tf.summary.scalar('accuracy', self.net.accuracy)\n",
        "\n",
        "        self.optimizer = self._get_optimizer(training_iters, global_step)\n",
        "        tf.summary.scalar('learning_rate', self.learning_rate_node)\n",
        "\n",
        "        self.summary_op = tf.summary.merge_all()\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        self.prediction_path = prediction_path\n",
        "        abs_prediction_path = os.path.abspath(self.prediction_path)\n",
        "        output_path = os.path.abspath(output_path)\n",
        "\n",
        "        if not restore:\n",
        "            logging.info(\"Removing '{:}'\".format(abs_prediction_path))\n",
        "            shutil.rmtree(abs_prediction_path, ignore_errors=True)\n",
        "            logging.info(\"Removing '{:}'\".format(output_path))\n",
        "            shutil.rmtree(output_path, ignore_errors=True)\n",
        "\n",
        "        if not os.path.exists(abs_prediction_path):\n",
        "            logging.info(\"Allocating '{:}'\".format(abs_prediction_path))\n",
        "            os.makedirs(abs_prediction_path)\n",
        "\n",
        "        if not os.path.exists(output_path):\n",
        "            logging.info(\"Allocating '{:}'\".format(output_path))\n",
        "            os.makedirs(output_path)\n",
        "\n",
        "        return init\n",
        "\n",
        "    def train(self, data_provider, output_path, training_iters=10, epochs=100, dropout=0.75, display_step=1,\n",
        "              restore=False, write_graph=False, prediction_path='prediction'):\n",
        "        \"\"\"\n",
        "        Lauches the training process\n",
        "        :param data_provider: callable returning training and verification data\n",
        "        :param output_path: path where to store checkpoints\n",
        "        :param training_iters: number of training mini batch iteration\n",
        "        :param epochs: number of epochs\n",
        "        :param dropout: dropout probability\n",
        "        :param display_step: number of steps till outputting stats\n",
        "        :param restore: Flag if previous model should be restored\n",
        "        :param write_graph: Flag if the computation graph should be written as protobuf file to the output path\n",
        "        :param prediction_path: path where to save predictions on each epoch\n",
        "        \"\"\"\n",
        "        save_path = os.path.join(output_path, \"model.ckpt\")\n",
        "        if epochs == 0:\n",
        "            return save_path\n",
        "\n",
        "        init = self._initialize(training_iters, output_path, restore, prediction_path)\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            if write_graph:\n",
        "                tf.train.write_graph(sess.graph_def, output_path, \"graph.pb\", False)\n",
        "\n",
        "            sess.run(init)\n",
        "\n",
        "            if restore:\n",
        "                ckpt = tf.train.get_checkpoint_state(output_path)\n",
        "                if ckpt and ckpt.model_checkpoint_path:\n",
        "                    self.net.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "            test_x, test_y = data_provider(self.verification_batch_size)\n",
        "            pred_shape = self.store_prediction(sess, test_x, test_y, \"_init\")\n",
        "\n",
        "            summary_writer = tf.summary.FileWriter(output_path, graph=sess.graph)\n",
        "            logging.info(\"Start optimization\")\n",
        "\n",
        "            avg_gradients = None\n",
        "            for epoch in range(epochs):\n",
        "                total_loss = 0\n",
        "                for step in range((epoch * training_iters), ((epoch + 1) * training_iters)):\n",
        "                    batch_x, batch_y = data_provider(self.batch_size)\n",
        "\n",
        "                    # Run optimization op (backprop)\n",
        "                    _, loss, lr, gradients = sess.run(\n",
        "                        (self.optimizer, self.net.cost, self.learning_rate_node, self.net.gradients_node),\n",
        "                        feed_dict={self.net.x: batch_x,\n",
        "                                   self.net.y: util.crop_to_shape(batch_y, pred_shape),\n",
        "                                   self.net.keep_prob: dropout})\n",
        "\n",
        "                    if self.net.summaries and self.norm_grads:\n",
        "                        avg_gradients = _update_avg_gradients(avg_gradients, gradients, step)\n",
        "                        norm_gradients = [np.linalg.norm(gradient) for gradient in avg_gradients]\n",
        "                        self.norm_gradients_node.assign(norm_gradients).eval()\n",
        "\n",
        "                    if step % display_step == 0:\n",
        "                        self.output_minibatch_stats(sess, summary_writer, step, batch_x,\n",
        "                                                    util.crop_to_shape(batch_y, pred_shape))\n",
        "\n",
        "                    total_loss += loss\n",
        "\n",
        "                self.output_epoch_stats(epoch, total_loss, training_iters, lr)\n",
        "                self.store_prediction(sess, test_x, test_y, \"epoch_%s\" % epoch)\n",
        "\n",
        "                save_path = self.net.save(sess, save_path)\n",
        "            logging.info(\"Optimization Finished!\")\n",
        "\n",
        "            return save_path\n",
        "\n",
        "    def store_prediction(self, sess, batch_x, batch_y, name):\n",
        "        prediction = sess.run(self.net.predicter, feed_dict={self.net.x: batch_x,\n",
        "                                                             self.net.y: batch_y,\n",
        "                                                             self.net.keep_prob: 1.})\n",
        "        pred_shape = prediction.shape\n",
        "\n",
        "        loss = sess.run(self.net.cost, feed_dict={self.net.x: batch_x,\n",
        "                                                  self.net.y: util.crop_to_shape(batch_y, pred_shape),\n",
        "                                                  self.net.keep_prob: 1.})\n",
        "\n",
        "        logging.info(\"Verification error= {:.1f}%, loss= {:.4f}\".format(error_rate(prediction,\n",
        "                                                                                   util.crop_to_shape(batch_y,\n",
        "                                                                                                      prediction.shape)),\n",
        "                                                                        loss))\n",
        "\n",
        "        img = util.combine_img_prediction(batch_x, batch_y, prediction)\n",
        "        util.save_image(img, \"%s/%s.jpg\" % (self.prediction_path, name))\n",
        "\n",
        "        return pred_shape\n",
        "\n",
        "    def output_epoch_stats(self, epoch, total_loss, training_iters, lr):\n",
        "        logging.info(\n",
        "            \"Epoch {:}, Average loss: {:.4f}, learning rate: {:.4f}\".format(epoch, (total_loss / training_iters), lr))\n",
        "\n",
        "    def output_minibatch_stats(self, sess, summary_writer, step, batch_x, batch_y):\n",
        "        # Calculate batch loss and accuracy\n",
        "        summary_str, loss, acc, predictions = sess.run([self.summary_op,\n",
        "                                                        self.net.cost,\n",
        "                                                        self.net.accuracy,\n",
        "                                                        self.net.predicter],\n",
        "                                                       feed_dict={self.net.x: batch_x,\n",
        "                                                                  self.net.y: batch_y,\n",
        "                                                                  self.net.keep_prob: 1.})\n",
        "        summary_writer.add_summary(summary_str, step)\n",
        "        summary_writer.flush()\n",
        "        logging.info(\n",
        "            \"Iter {:}, Minibatch Loss= {:.4f}, Training Accuracy= {:.4f}, Minibatch error= {:.1f}%\".format(step,\n",
        "                                                                                                           loss,\n",
        "                                                                                                           acc,\n",
        "                                                                                                           error_rate(\n",
        "                                                                                                               predictions,\n",
        "                                                                                                               batch_y)))\n",
        "\n",
        "\n",
        "def _update_avg_gradients(avg_gradients, gradients, step):\n",
        "    if avg_gradients is None:\n",
        "        avg_gradients = [np.zeros_like(gradient) for gradient in gradients]\n",
        "    for i in range(len(gradients)):\n",
        "        avg_gradients[i] = (avg_gradients[i] * (1.0 - (1.0 / (step + 1)))) + (gradients[i] / (step + 1))\n",
        "\n",
        "    return avg_gradients\n",
        "\n",
        "\n",
        "def error_rate(predictions, labels):\n",
        "    \"\"\"\n",
        "    Return the error rate based on dense predictions and 1-hot labels.\n",
        "    \"\"\"\n",
        "\n",
        "    return 100.0 - (\n",
        "            100.0 *\n",
        "            np.sum(np.argmax(predictions, 3) == np.argmax(labels, 3)) /\n",
        "            (predictions.shape[0] * predictions.shape[1] * predictions.shape[2]))\n",
        "\n",
        "\n",
        "def get_image_summary(img, idx=0):\n",
        "    \"\"\"\n",
        "    Make an image summary for 4d tensor image with index idx\n",
        "    \"\"\"\n",
        "\n",
        "    V = tf.slice(img, (0, 0, 0, idx), (1, -1, -1, 1))\n",
        "    V -= tf.reduce_min(V)\n",
        "    V /= tf.reduce_max(V)\n",
        "    V *= 255\n",
        "\n",
        "    img_w = tf.shape(img)[1]\n",
        "    img_h = tf.shape(img)[2]\n",
        "    V = tf.reshape(V, tf.stack((img_w, img_h, 1)))\n",
        "    V = tf.transpose(V, (2, 0, 1))\n",
        "    V = tf.reshape(V, tf.stack((-1, img_w, img_h, 1)))\n",
        "    return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gSSP3UAZSlB",
        "colab_type": "text"
      },
      "source": [
        "Ref:\\\n",
        "https://github.com/YBIGTA/data-science-2018/blob/master/DLCV/2018-02-10-Keras-U-Net-starter.md\n",
        "\\\n",
        "\n",
        "https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiF2bbBEemLS",
        "colab_type": "text"
      },
      "source": [
        "copied from:\\\n",
        "https://github.com/zhixuhao/unet/blob/master/model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo4bXZZ4ev-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import numpy as np\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as keras\n",
        "\n",
        "\n",
        "def unet(pretrained_weights = None,input_size = (256,256,1)):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = Model(input = inputs, output = conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    model.summary()\n",
        "\n",
        "    if(pretrained_weights):\n",
        "    \tmodel.load_weights(pretrained_weights)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}