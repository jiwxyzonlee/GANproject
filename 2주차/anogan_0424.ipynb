{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anogan_0424.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R2iS2vZ_4XO",
        "colab_type": "text"
      },
      "source": [
        "copied and edited from:\\\n",
        "https://github.com/tkwoo/anogan-keras/blob/master/anogan.py\n",
        "\n",
        "copied edited from:\\\n",
        "https://github.com/yjucho1/anoGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-cSFN4_3xL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Reshape, Dense, Dropout, MaxPooling2D, Conv2D, Flatten\n",
        "from keras.layers import Conv2DTranspose, LeakyReLU\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "from keras.utils. generic_utils import Progbar\n",
        "\n",
        "### combine images for visualization\n",
        "def combine_images(generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[1:4]\n",
        "    image = np.zeros((height*shape[0], width*shape[1], shape[2]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1],:] = img[:, :, :]\n",
        "    return image\n",
        "\n",
        "### generator model define\n",
        "def generator_model():\n",
        "    inputs = Input((10,))\n",
        "    fc1 = Dense(input_dim=10, units=128*7*7)(inputs)\n",
        "    fc1 = BatchNormalization()(fc1)\n",
        "    fc1 = LeakyReLU(0.2)(fc1)\n",
        "    fc2 = Reshape((7, 7, 128), input_shape=(128*7*7,))(fc1)\n",
        "    up1 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(fc2)\n",
        "    conv1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    up2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv1)\n",
        "    conv2 = Conv2D(1, (5, 5), padding='same')(up2)\n",
        "    outputs = Activation('tanh')(conv2)\n",
        "    \n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "### discriminator model define\n",
        "def discriminator_model():\n",
        "    inputs = Input((28, 28, 1))\n",
        "    conv1 = Conv2D(64, (5, 5), padding='same')(inputs)\n",
        "    conv1 = LeakyReLU(0.2)(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, (5, 5), padding='same')(pool1)\n",
        "    conv2 = LeakyReLU(0.2)(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    fc1 = Flatten()(pool2)\n",
        "    fc1 = Dense(1)(fc1)\n",
        "    outputs = Activation('sigmoid')(fc1)\n",
        "    \n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "### d_on_g model for training generator\n",
        "def generator_containing_discriminator(g, d):\n",
        "    d.trainable = False\n",
        "    ganInput = Input(shape=(10,))\n",
        "    x = g(ganInput)\n",
        "    ganOutput = d(x)\n",
        "    gan = Model(inputs=ganInput, outputs=ganOutput)\n",
        "    # gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan\n",
        "\n",
        "def load_model():\n",
        "    d = discriminator_model()\n",
        "    g = generator_model()\n",
        "    d_optim = RMSprop()\n",
        "    g_optim = RMSprop(lr=0.0002)\n",
        "    g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
        "    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
        "    d.load_weights('./weights/discriminator.h5')\n",
        "    g.load_weights('./weights/generator.h5')\n",
        "    return g, d\n",
        "\n",
        "### train generator and discriminator\n",
        "def train(BATCH_SIZE, X_train):\n",
        "    \n",
        "    ### model define\n",
        "    d = discriminator_model()\n",
        "    g = generator_model()\n",
        "    d_on_g = generator_containing_discriminator(g, d)\n",
        "    d_optim = RMSprop(lr=0.0004)\n",
        "    g_optim = RMSprop(lr=0.0002)\n",
        "    g.compile(loss='mse', optimizer=g_optim)\n",
        "    d_on_g.compile(loss='mse', optimizer=g_optim)\n",
        "    d.trainable = True\n",
        "    d.compile(loss='mse', optimizer=d_optim)\n",
        "    \n",
        "\n",
        "    for epoch in range(10):\n",
        "        print (\"Epoch is\", epoch)\n",
        "        n_iter = int(X_train.shape[0]/BATCH_SIZE)\n",
        "        progress_bar = Progbar(target=n_iter)\n",
        "        \n",
        "        for index in range(n_iter):\n",
        "            # create random noise -> U(0,1) 10 latent vectors\n",
        "            noise = np.random.uniform(0, 1, size=(BATCH_SIZE, 10))\n",
        "\n",
        "            # load real data & generate fake data\n",
        "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "            generated_images = g.predict(noise, verbose=0)\n",
        "            \n",
        "            # visualize training results\n",
        "            if index % 20 == 0:\n",
        "                image = combine_images(generated_images)\n",
        "                image = image*127.5+127.5\n",
        "                cv2.imwrite('./result/'+str(epoch)+\"_\"+str(index)+\".png\", image)\n",
        "\n",
        "            # attach label for training discriminator\n",
        "            X = np.concatenate((image_batch, generated_images))\n",
        "            y = np.array([1] * BATCH_SIZE + [0] * BATCH_SIZE)\n",
        "            \n",
        "            # training discriminator\n",
        "            d_loss = d.train_on_batch(X, y)\n",
        "\n",
        "            # training generator\n",
        "            d.trainable = False\n",
        "            g_loss = d_on_g.train_on_batch(noise, np.array([1] * BATCH_SIZE))\n",
        "            d.trainable = True\n",
        "\n",
        "            progress_bar.update(index, values=[('g',g_loss), ('d',d_loss)])\n",
        "        print ('')\n",
        "\n",
        "        # save weights for each epoch\n",
        "        g.save_weights('weights/generator.h5', True)\n",
        "        d.save_weights('weights/discriminator.h5', True)\n",
        "    return d, g\n",
        "\n",
        "### generate images\n",
        "def generate(BATCH_SIZE):\n",
        "    g = generator_model()\n",
        "    g.load_weights('weights/generator.h5')\n",
        "    noise = np.random.uniform(0, 1, (BATCH_SIZE, 10))\n",
        "    generated_images = g.predict(noise)\n",
        "    return generated_images\n",
        "\n",
        "### anomaly loss function \n",
        "def sum_of_residual(y_true, y_pred):\n",
        "    return K.sum(K.abs(y_true - y_pred))\n",
        "\n",
        "### discriminator intermediate layer feautre extraction\n",
        "def feature_extractor(d=None):\n",
        "    if d is None:\n",
        "        d = discriminator_model()\n",
        "        d.load_weights('weights/discriminator.h5') \n",
        "    intermidiate_model = Model(inputs=d.layers[0].input, outputs=d.layers[-7].output)\n",
        "    intermidiate_model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "    return intermidiate_model\n",
        "\n",
        "### anomaly detection model define\n",
        "def anomaly_detector(g=None, d=None):\n",
        "    if g is None:\n",
        "        g = generator_model()\n",
        "        g.load_weights('weights/generator.h5')\n",
        "    intermidiate_model = feature_extractor(d)\n",
        "    intermidiate_model.trainable = False\n",
        "    g = Model(inputs=g.layers[1].input, outputs=g.layers[-1].output)\n",
        "    g.trainable = False\n",
        "    # Input layer cann't be trained. Add new layer as same size & same distribution\n",
        "    aInput = Input(shape=(10,))\n",
        "    gInput = Dense((10), trainable=True)(aInput)\n",
        "    gInput = Activation('sigmoid')(gInput)\n",
        "    \n",
        "    # G & D feature\n",
        "    G_out = g(gInput)\n",
        "    D_out= intermidiate_model(G_out)    \n",
        "    model = Model(inputs=aInput, outputs=[G_out, D_out])\n",
        "    model.compile(loss=sum_of_residual, loss_weights= [0.90, 0.10], optimizer='rmsprop')\n",
        "    \n",
        "    # batchnorm learning phase fixed (test) : make non trainable\n",
        "    K.set_learning_phase(0)\n",
        "    \n",
        "    return model\n",
        "\n",
        "### anomaly detection\n",
        "def compute_anomaly_score(model, x, iterations=500, d=None):\n",
        "    z = np.random.uniform(0, 1, size=(1, 10))\n",
        "    \n",
        "    intermidiate_model = feature_extractor(d)\n",
        "    d_x = intermidiate_model.predict(x)\n",
        "\n",
        "    # learning for changing latent\n",
        "    loss = model.fit(z, [x, d_x], batch_size=1, epochs=iterations, verbose=0)\n",
        "    similar_data, _ = model.predict(z)\n",
        "    \n",
        "    loss = loss.history['loss'][-1]\n",
        "    \n",
        "    return loss, similar_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDb-l1shIiBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DD9YXCnIiD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Reshape, Dense, Dropout, UpSampling2D, Conv2D, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generator_model():\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(128*7*7, input_dim=100, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "    generator.add(Reshape((7, 7, 128)))\n",
        "    generator.add(UpSampling2D(size=(2, 2)))\n",
        "    generator.add(Conv2D(64, kernel_size=(5, 5), padding='same'))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "    generator.add(UpSampling2D(size=(2, 2)))\n",
        "    generator.add(Conv2D(1, kernel_size=(5, 5), padding='same', activation='tanh'))\n",
        "    generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return generator\n",
        "\n",
        "\n",
        "def discriminator_model():\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(28,28, 1), kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        "    discriminator.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        "    discriminator.add(Flatten())\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return discriminator\n",
        "\n",
        "\n",
        "def generator_containing_discriminator(g, d):\n",
        "    d.trainable = False\n",
        "    ganInput = Input(shape=(100,))\n",
        "    x = g(ganInput)\n",
        "    ganOutput = d(x)\n",
        "    gan = Model(inputs=ganInput, outputs=ganOutput)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan\n",
        "\n",
        "def train(BATCH_SIZE, X_train):\n",
        "    d = discriminator_model()\n",
        "    print(\"#### discriminator ######\")\n",
        "    d.summary()\n",
        "    g = generator_model()\n",
        "    print(\"#### generator ######\")\n",
        "    g.summary()\n",
        "    d_on_g = generator_containing_discriminator(g, d)\n",
        "    d.trainable = True\n",
        "    for epoch in tqdm(range(200)):\n",
        "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
        "            noise = np.random.uniform(0, 1, size=(BATCH_SIZE, 100))\n",
        "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "            generated_images = g.predict(noise, verbose=0)\n",
        "            X = np.concatenate((image_batch, generated_images))\n",
        "            y = np.array([1] * BATCH_SIZE + [0] * BATCH_SIZE)\n",
        "            d_loss = d.train_on_batch(X, y)\n",
        "            noise = np.random.uniform(0, 1, (BATCH_SIZE, 100))\n",
        "            d.trainable = False\n",
        "            g_loss = d_on_g.train_on_batch(noise, np.array([1] * BATCH_SIZE))\n",
        "            d.trainable = True\n",
        "        g.save_weights('assets/generator', True)\n",
        "        d.save_weights('assets/discriminator', True)\n",
        "    return d, g\n",
        "\n",
        "\n",
        "def generate(BATCH_SIZE):\n",
        "    g = generator_model()\n",
        "    g.load_weights('assets/generator')\n",
        "    noise = np.random.uniform(0, 1, (BATCH_SIZE, 100))\n",
        "    generated_images = g.predict(noise)\n",
        "    return generated_images\n",
        "\n",
        "def sum_of_residual(y_true, y_pred):\n",
        "    return tf.reduce_sum(abs(y_true - y_pred))\n",
        "\n",
        "def feature_extractor():\n",
        "    d = discriminator_model()\n",
        "    d.load_weights('assets/discriminator') \n",
        "    intermidiate_model = Model(inputs=d.layers[0].input, outputs=d.layers[-5].output)\n",
        "    intermidiate_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return intermidiate_model\n",
        "\n",
        "def anomaly_detector():\n",
        "    g = generator_model()\n",
        "    g.load_weights('assets/generator')\n",
        "    g.trainable = False\n",
        "    intermidiate_model = feature_extractor()\n",
        "    intermidiate_model.trainable = False\n",
        "    \n",
        "    aInput = Input(shape=(100,))\n",
        "    gInput = Dense((100))(aInput)\n",
        "    G_out = g(gInput)\n",
        "    D_out= intermidiate_model(G_out)    \n",
        "    model = Model(inputs=aInput, outputs=[G_out, D_out])\n",
        "    model.compile(loss=sum_of_residual, loss_weights= [0.9, 0.1], optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def compute_anomaly_score(model, x):    \n",
        "    z = np.random.uniform(0, 1, size=(1, 100))\n",
        "    intermidiate_model = feature_extractor()\n",
        "    d_x = intermidiate_model.predict(x)\n",
        "    loss = model.fit(z, [x, d_x], epochs=500, verbose=0)\n",
        "    similar_data, _ = model.predict(z)\n",
        "    return loss.history['loss'][-1], similar_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep01G3fiFhlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from tqdm import tqdm\n",
        "# import anogan\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.astype(np.float32)/255.\n",
        "X_train = X_train.reshape(60000, 28, 28, 1)\n",
        "\n",
        "\n",
        "Model_d, Model_g = anogan.train(32, X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0NZr5OyFhnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## generate random image \n",
        "\n",
        "generated_img = anogan.generate(3)\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(generated_img[0].reshape(28, 28),cmap=plt.cm.gray)\n",
        "plt.show("
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKiM0Ic6Fhpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## compute anomaly score - sample from test set\n",
        "\n",
        "X_test = X_test.astype(np.float32)/255.\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "test_img = X_test[0]\n",
        "\n",
        "model = anogan.anomaly_detector()\n",
        "ano_score, similar_img = anogan.compute_anomaly_score(model, test_img.reshape(1, 28, 28, 1))\n",
        "\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(test_img.reshape(28,28), cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "print(\"anomaly score : \" + str(ano_score))\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(test_img.reshape(28,28), cmap=plt.cm.gray)\n",
        "residual  = test_img.reshape(28,28) - similar_img.reshape(28, 28)\n",
        "plt.imshow(residual, cmap='jet', alpha=.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvbGhuBzFhru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## compute anomaly score - sample from strange image\n",
        "\n",
        "test_img = plt.imread('assets/test_img.png')\n",
        "test_img = test_img[:,:,0]\n",
        "\n",
        "model = anogan.anomaly_detector()\n",
        "ano_score, similar_img = anogan.compute_anomaly_score(model, test_img.reshape(1, 28, 28, 1))\n",
        "\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(test_img.reshape(28,28), cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "print(\"anomaly score : \" + str(ano_score))\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(test_img.reshape(28,28), cmap=plt.cm.gray)\n",
        "residual  = test_img.reshape(28,28) - similar_img.reshape(28, 28)\n",
        "plt.imshow(residual, cmap='jet', alpha=.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QLGzeyfFht2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "## t-SNE embedding \n",
        "\n",
        "# generating anomaly image for test (radom noise image)\n",
        "\n",
        "random_image = np.random.uniform(0,1, (100, 28,28, 1))\n",
        "print(\"a sample from generated anomaly images(random noise image)\")\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(random_image[0].reshape(28,28), cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "\n",
        "# intermidieate output of discriminator\n",
        "model = anogan.feature_extractor()\n",
        "feature_map_of_random = model.predict(random_image, verbose=1)\n",
        "feature_map_of_minist = model.predict(X_test[:300], verbose=1)\n",
        "\n",
        "# t-SNE for visulization\n",
        "output = np.concatenate((feature_map_of_random, feature_map_of_minist))\n",
        "output = output.reshape(output.shape[0], -1)\n",
        "anomaly_flag = np.array([1]*100+ [0]*300)\n",
        "\n",
        "X_embedded = TSNE(n_components=2).fit_transform(output)\n",
        "plt.title(\"t-SNE embedding on the feature representation\")\n",
        "plt.scatter(X_embedded[:100,0], X_embedded[:100,1], label='random noise(anomaly)')\n",
        "plt.scatter(X_embedded[100:,0], X_embedded[100:,1], label='minist(normal)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUiO3d6eFqWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gVQ9C90FqY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WztYi5QFqbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}